{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6416988b82e3479a8e2bd90bc3871668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1606274776606_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-15-216.us-east-2.compute.internal:20888/proxy/application_1606274776606_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-10-129.us-east-2.compute.internal:8042/node/containerlogs/container_1606274776606_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sparkdl\n",
      "  Using cached https://files.pythonhosted.org/packages/70/e6/c520f801b945f3d03dbf47e1abb7a454cda328d1592f9854dcec69bed097/sparkdl-0.2.2-py3-none-any.whl\n",
      "Installing collected packages: sparkdl\n",
      "Successfully installed sparkdl-0.2.2\n",
      "\n",
      "Collecting torch\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting future (from torch)\n",
      "Requirement already satisfied: numpy in /usr/local/lib64/python3.7/site-packages (from torch)\n",
      "Collecting typing-extensions (from torch)\n",
      "  Using cached https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Collecting dataclasses (from torch)\n",
      "  Using cached https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
      "Installing collected packages: future, typing-extensions, dataclasses, torch\n",
      "Successfully installed dataclasses-0.6 future-0.18.2 torch-1.7.0 typing-extensions-3.7.4.3\n",
      "\n",
      "Collecting torchvision\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/39/a9caac0deb027feec2cdd7cc40b2a598256d3f50050c80f349c030f915f2/torchvision-0.8.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting pillow>=4.1.1 (from torchvision)\n",
      "  Using cached https://files.pythonhosted.org/packages/af/fa/c1302a26d5e1a17fa8e10e43417b6cf038b0648c4b79fcf2302a4a0c5d30/Pillow-8.0.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: torch==1.7.0 in /mnt/tmp/1606275766405-0/lib/python3.7/site-packages (from torchvision)\n",
      "Requirement already satisfied: numpy in /usr/local/lib64/python3.7/site-packages (from torchvision)\n",
      "Requirement already satisfied: future in /mnt/tmp/1606275766405-0/lib/python3.7/site-packages (from torch==1.7.0->torchvision)\n",
      "Requirement already satisfied: typing-extensions in /mnt/tmp/1606275766405-0/lib/python3.7/site-packages (from torch==1.7.0->torchvision)\n",
      "Requirement already satisfied: dataclasses in /mnt/tmp/1606275766405-0/lib/python3.7/site-packages (from torch==1.7.0->torchvision)\n",
      "Installing collected packages: pillow, torchvision\n",
      "Successfully installed pillow-8.0.1 torchvision-0.8.1\n",
      "\n",
      "Collecting PyArrow\n",
      "  Using cached https://files.pythonhosted.org/packages/c8/58/d07e7ee8b0cffe509f9e5a3742e09636a4a58b2113d193166615b934846f/pyarrow-2.0.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/lib64/python3.7/site-packages (from PyArrow)\n",
      "Installing collected packages: PyArrow\n",
      "Successfully installed PyArrow-2.0.0"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc =SparkContext.getOrCreate()\n",
    "#sc.install_pypi_package(\"pandas==0.25.1\") #Install pandas version 0.25.1 \n",
    "sc.install_pypi_package(\"sparkdl\")\n",
    "sc.install_pypi_package(\"torch\")\n",
    "sc.install_pypi_package(\"torchvision\")\n",
    "sc.install_pypi_package(\"PyArrow\")\n",
    "\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets.folder import default_loader  # private API\n",
    "import os\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "Params = namedtuple('Params', ['batch_size', 'test_batch_size', 'epochs', 'lr', 'momentum', 'seed', 'cuda', 'log_interval'])\n",
    "args = Params(batch_size=64, test_batch_size=64, epochs=1, lr=0.001, momentum=0.5, seed=1, cuda=use_cuda, log_interval=20)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_mri_path = 's3://braintumorproject/Healthcare_AI_Datasets/Brain_MRI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = [brain_mri_path+a[0].replace('.tif','.png') for a in paths]\n",
    "        image1 = spark.read.format(\"image\").option(\"dropInvalid\", False).load(self.paths)\n",
    "        images = image1.collect()\n",
    "        Xt = []\n",
    "        for i in range(len(images)):\n",
    "            w = images[i][0]\n",
    "            image2 = ImageSchema.toNDArray(w)\n",
    "            Xt.append((w[0].split('/')[-1],image2))\n",
    "        self.images = sorted(Xt, key=lambda tup: tup[0])\n",
    "        yt = [(a[0].split('/')[-1],a[1]) for a in paths]\n",
    "        self.labels = sorted(yt, key=lambda tup: tup[0])\n",
    "\n",
    "\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return  len(self.paths)\n",
    "    def __getitem__(self, index):\n",
    "        #if npg exist, skip the following steps\n",
    "        #im = Image.open('Healthcare_AI_Datasets/Brain_MRI/'+self.paths[index][0])\n",
    "        #rgb_im = im.convert('RGB')\n",
    "\n",
    "        #im.save('Healthcare_AI_Datasets/Brain_MRI/'+self.paths[index][0].replace(\".tif\", \".png\"), quality=95)\n",
    "        \n",
    "        \n",
    "        #data is already loaded\n",
    "        image1 = self.images[index][1]\n",
    "\n",
    "        #read one by one\n",
    "        #image1 = spark.read.format(\"image\").option(\"dropInvalid\", False).load('Healthcare_AI_Datasets/Brain_MRI/'+ self.paths[index][0].replace(\".tif\", \".png\")).collect()\n",
    "        #image1 = image1[0][0]\n",
    "        \n",
    "        #image1 = ImageSchema.readImages('Healthcare_AI_Datasets/Brain_MRI/'+ self.paths[index][0].replace(\".tif\", \".png\")).collect()\n",
    "        #image2 = ImageSchema.toNDArray(image1)\n",
    "        #image = np.transpose(image2, (2, 1, 0))\n",
    "        if self.transform is not None:\n",
    "            image3 = Image.fromarray(image1.astype('uint8'), 'RGB')\n",
    "            image = self.transform(image3)\n",
    "        return image, self.labels[index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mask_path = 's3://braintumorproject/Healthcare_AI_Datasets/Brain_MRI/data_mask.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"how to read csv file\").config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "df = spark.read.option(\"header\",\"True\").csv(data_mask_path)\n",
    "df = df.select(\"image_path\",\"mask\")\n",
    "list_files = df.collect()\n",
    "list_files = [(list_files[i].image_path, int(list_files[i].mask)) for i in range(len(list_files))]\n",
    "\n",
    "from random import shuffle\n",
    "shuffle(list_files)\n",
    "Train_size = int(len(list_files)* 0.85)\n",
    "list_files_train = list_files[:Train_size]\n",
    "list_files_test = list_files[Train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #self.bnf = nn.BatchNorm1d(2048)\n",
    "        self.fc0 = nn.Linear(2048, 512)\n",
    "        self.fc1 = nn.Linear(512,256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.bnf(x)\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.fc2(x)\n",
    "        x= self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = Net()\n",
    "model.share_memory()\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Sequential(nn.Linear(num_ftrs, 512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(p=0.3),\n",
    "                                nn.Linear(512, 256),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(p=0.3),\n",
    "                                nn.Linear(256, 2))\n",
    "\n",
    "for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "cross_loss = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(224),\n",
    "        #transforms.RandomRotation(degrees=(-90, 90)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        transforms.Normalize(mean=[0.08, 0.08, 0.08],std=[0.12, 0.12, 0.12]) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(224),\n",
    "        #transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        transforms.Normalize(mean=[0.08, 0.08, 0.08],std=[0.12, 0.12, 0.12])               \n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(ImageDataset(list_files_train,train_transform),\n",
    "        batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader( ImageDataset(list_files_test,test_transform),\n",
    "        batch_size=args.test_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, args, model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()      \n",
    "        #for _ in range(1):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = cross_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), train_loss/(batch_idx+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            if args.cuda:\n",
    "                data, target = data.cuda(), target.cuda()      \n",
    "            output = model(data)\n",
    "            test_loss += cross_loss(output, target).data.item() # sum up batch loss\n",
    "            pred = output.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum().item()\n",
    "\n",
    "        #test_loss /= len(data_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset),\n",
    "            100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    \n",
    "    train_epoch(epoch, args, model, train_loader, optimizer)\n",
    "    test_epoch(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
